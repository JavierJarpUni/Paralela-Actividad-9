# Calculadora de Promedio Distribuido con MPI
Este repositorio contiene un programa en C que calcula el promedio de valores generados aleatoriamente de forma distribuida entre m√∫ltiples procesos, utilizando la interfaz de paso de mensajes (MPI) para la comunicaci√≥n colectiva. Sirve como una demostraci√≥n pr√°ctica del uso de MPI_Bcast y MPI_Reduce.

## üöÄ Caracter√≠sticas
- Generaci√≥n de Datos Distribuida: Cada proceso genera su propio conjunto de n√∫meros aleatorios.
- Distribuci√≥n de Par√°metros (MPI_Bcast): El proceso ra√≠z distribuye el n√∫mero N de valores a generar a todos los dem√°s procesos.
- Agregaci√≥n de Sumas (MPI_Reduce): Las sumas parciales de cada proceso se agregan en el proceso ra√≠z para obtener una suma total.
- Distribuci√≥n del Promedio Final (MPI_Bcast): El promedio calculado por el proceso ra√≠z se distribuye a todos los procesos para su visualizaci√≥n.
- Sincronizaci√≥n Impl√≠cita: Demuestra c√≥mo las operaciones colectivas de MPI act√∫an como puntos de sincronizaci√≥n naturales.

## üìã Requisitos

Para compilar y ejecutar este programa, necesitas tener una implementaci√≥n de MPI instalada en tu sistema. 
Las opciones populares incluyen:
- Open MPI (recomendado y ampliamente utilizado)
- MPICH

### Instalaci√≥n de MPI (Ejemplo para macOS con Homebrew)

Si usas macOS, puedes instalar Open MPI f√°cilmente a trav√©s de Homebrew:

1. Instala las Herramientas de L√≠nea de Comandos de Xcode:
```bash  
xcode-select --install
```
2. Instala Homebrew (si no lo tienes):
```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

3. Instala Open MPI:
```bash
brew install open-mpi
```
Para otros sistemas operativos (Linux, Windows con WSL), consulta la documentaci√≥n oficial de Open MPI o MPICH para instrucciones de instalaci√≥n.

## üõ†Ô∏è Compilaci√≥n

Navega hasta el directorio donde has guardado average_calculator.c y compila el programa utilizando el compilador wrapper de MPI (mpicc):
```bash
mpicc average_calculator.c -o average_calculator
```
Esto generar√° un archivo ejecutable llamado average_calculator.

## üöÄ Ejecuci√≥n
Para ejecutar el programa y aprovechar el paralelismo, utiliza el comando mpirun (o mpiexec), especificando el n√∫mero de procesos (-np) que deseas utilizar.Cuando inicies el programa, el proceso ra√≠z (Rank 0) te pedir√° que ingreses un valor entero positivo para N (la cantidad de valores aleatorios que cada proceso generar√°).

- Ejecutar con 2 procesos:
```bash
mpirun -np 2 ./average_calculator
```

- Ejecutar con 4 procesos:
```bash
mpirun -np 4 ./average_calculator
```

- Ejecutar con X procesos (reemplaza X por el n√∫mero deseado):
```bash
mpirun -np X ./average_calculator
```
- Ejemplo de Salida (con N=3 y 2 procesos):

```bash
Proceso Raiz (Rank 0): Ingrese el tama√±o N (cantidad de valores por proceso): 3
Proceso Raiz (Rank 0): N = 3. Distribuyendo N a todos los procesos.
Proceso 0: N recibido = 3.
Proceso 1: N recibido = 3.
Proceso 0: Generando 3 valores aleatorios...
Proceso 1: Generando 3 valores aleatorios...
Proceso 1: Suma parcial calculada = 143.512345
Proceso 0: Suma parcial calculada = 105.234567
Proceso Raiz (Rank 0): Suma total de todos los valores = 248.746912
Proceso Raiz (Rank 0): Promedio total calculado = 41.457819
Proceso 1: Promedio total recibido = 41.457819
Proceso 0: Promedio total recibido = 41.457819
```
üß† ¬øC√≥mo funciona?

1. Inicializaci√≥n: Todos los procesos se inician y obtienen su rank (ID √∫nico) y num_procs (total de procesos).
2. Input N & Broadcast: El proceso con rank 0 (ra√≠z) solicita el valor N al usuario y lo env√≠a a todos los dem√°s procesos utilizando MPI_Bcast.
3. Trabajo Local: Cada proceso, ahora conociendo N, genera N n√∫meros aleatorios localmente y calcula su partial_sum.
4. Reducci√≥n: Todos los partial_sum se env√≠an al proceso ra√≠z y se suman colectivamente usando MPI_Reduce (operaci√≥n MPI_SUM). El resultado (total_sum) reside en el proceso ra√≠z.
5. C√°lculo y Broadcast del Promedio Final: El proceso ra√≠z calcula el promedio global (total_sum / (N * num_procs)) y lo distribuye a todos los procesos usando otro MPI_Bcast.
6. Visualizaci√≥n: Cada proceso imprime el promedio final.
7. Finalizaci√≥n: Se cierran los recursos de MPI.

## üìö Documentaci√≥n Adicional

Para una explicaci√≥n m√°s profunda del dise√±o del programa, la justificaci√≥n del uso de cada operaci√≥n colectiva, y un an√°lisis de la sincronizaci√≥n y posibles puntos cr√≠ticos en aplicaciones MPI, por favor, consulta el informe completo del proyecto.

## üé• Video Explicativo

Un video demostrativo que explica el c√≥digo, su funcionamiento y los conceptos de MPI aplicados.
